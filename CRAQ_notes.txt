Object Stores-> Hierachical or flat namespaces?
 Object stores are better suited for flat names
paces, such as in key-value databases, as opposed to hi
erarchical directory structures. Object stores simplify the
 process of supporting whole-object modifications.
Also object stores only need to reason about the ordering of
 modifications to a specific object, as opposed to the en
tire storage system; it is significantly cheaper to provide
 consistency guarantees per object instead of across all op
erations and/or objects

What is atomic broadcast?
Atomic broadcast (also known as total order broadcast) is a fundamental concept in distributed systems, and it plays a crucial role in Apache ZooKeeper's operation.
In the context of ZooKeeper, atomic broadcast refers to a communication protocol that ensures all servers in the
ZooKeeper ensemble receive and process messages in exactly the same order. This is essential for maintaining consistency across the distributed system.
Key characteristics of atomic broadcast in ZooKeeper:

Total ordering: All messages are delivered to all servers in the same order, ensuring that all servers see the same sequence of operations.
Atomicity: Either all correct servers deliver a message, or none do. This prevents inconsistent states across the cluster.
Implementation via ZAB: ZooKeeper uses the ZooKeeper Atomic Broadcast (ZAB) protocol to implement atomic broadcast. 
ZAB ensures that updates to the system are delivered in the same order to all servers.
Leader-based approach: ZAB uses a leader-follower model where a single server (the leader) orders all update requests and then broadcasts them to followers.
Quorum-based consensus: Updates are considered committed only when a quorum (majority) of servers has received and acknowledged them.

This atomic broadcast mechanism is what enables ZooKeeper to provide its strong consistency guarantees, 
allowing it to function as a reliable coordination service for distributed applications despite potential network delays, partitions, or server failures.

Intuition for linearizability of CR?
  When no failures, almost as if the tail were the only server.
    Head picks an order for writes, replicas apply in that order,
      so they will stay in sync except for recent (uncommitted) writes.
    Tail exposes only committed writes to readers.
  Failure recovery, briefly.
    Good news: every replica knows of every committed write.
    But need to push partial writes down the chain.
    If head fails, successor takes over as head, no commited writes lost.
    If tail fails, predecessor takes over as tail, no writes lost.
    If intermediate fails, drop from chain, predecessor may need to
      re-send recent writes.

Why is CR attractive (vs Raft)?
  Client RPCs split between head and tail, vs Raft's leader handles both.
  Head sends each write just once, vs Raft's leader sends to all.
  Reads involve just one server, not all as in Raft.
  Situation after failure simpler than in Raft 

Why is it attractive to let clients read any replica in CR?
  The opportunity is that the intermediate replicas may still
    have spare CPU cycles when the read load is high enough to saturate the tail.
  Moving read load from the tail to the intermediate nodes
    might thus yield higher read throughput on a saturated chain.
  The CRAQ paper admits there is at least one other way to skin this cat:
    Split objects over many chains, each server participates in multiple chains.
    C1: S1 S2 S3
    C2: S2 S3 S1
    C3: S3 S1 S2
  This works if load is more or less evenly divided among chains.
  It often isn't.

Would it be correct (linearizable) to let clients read any replica in CR?
  No.
  A read could see uncommitted data, might disappear due to a failure.
  A client could see a new value from one replica,
    and then an older value from a different (later) replica.

Chain Replication is a distributed protocol(which provides strong consistency) where nodes are arranged in a linear chain. 
Write operations enter at the head of the chain and propagate sequentially down to the tail, while read operations are
traditionally served exclusively by the tail node.

The statement makes three important points:
Reads from any replica violate linearizability: In Chain Replication, linearizability (the strongest consistency model 
where operations appear to execute atomically and in real-time order) is guaranteed by restricting reads to the tail node. 
Reading from any replica breaks this guarantee.
Uncommitted data exposure risk: If clients can read from any replica, they might see data that has only been partially replicated through the chain.
Since a write is only considered committed when it reaches the tail, reading from an intermediate node could expose updates that might later be lost 
if that node fails before the update propagates fully. Read inconsistency between replicas: The most severe violation of linearizability 
comes from the possibility of temporal anomalies. A client might read a newer value from one replica, 
then later read an older value from a different replica (even though the second read happens chronologically after the first). 
This "going back in time" directly violates the real-time ordering requirement of linearizability.

This is precisely why classic Chain Replication enforces a strict access pattern: all writes go to the head, 
and all reads come from the tail. This design choice ensures that clients only see committed data in a consistent order, 
maintaining linearizability at the cost of focusing all read load on a single node (the tail).


Problem with CR across multiple datacentres?
 When attempting to build chains across multiple datacenters, as all reads to a chain
 may then be handled by a potentially-distant node (the chain’s tail) which would not provide low latency.
It reduces read throughput to that of a single node, instead of being able to scale out with chain size.

Apportioned queries-> dividing read operations over all nodes in a chain, as opposed to requiring that they all be handled by a single primary node. 

Monotonic Read Consistency->Successive reads to an object will return either the same prior value or a more recent one, but never an older value.

In CR-> We send writes from the head till the tail, when it reaches the tail it is said to be comitted, then we propagate the commit back to the head node
which replies to the client.

Why can CRAQ serve reads from replicas linearizably but Raft/ZooKeeper/&c cannot?
Relies on being a chain, so that all nodes see each write before the write commits, so nodes know about all writes that might have committed, 
and thus know when to ask the tail.
Raft/ZooKeeper can't do this because leader can proceed with a mere majority, so can commit without all followers seeing a write, so 
followers are not aware when they have missed a committed write.

CRAQ’s throughput improvements over CR arise in two different scenarios:
 • Read-Mostly Workloads have most of the read requests handled solely by the C-1 non-tail nodes (as clean reads), and thus throughput in these scenarios
 scales linearly with chain size C.
 • Write-Heavy Workloads have most read requests to non-tail nodes as dirty, thus require version queries
 to the tail. We suggest, however, that these version queries are lighter-weight than full reads, allowing the tail to process them at a much higher rate 
before it becomes saturated. This leads to a total read throughput that is still higher than CR.

How CRAQ works->
CRAQ nodes store multiple versions of an object, each with a version number and a clean/dirty status. Initially, all versions are clean.
When a node receives a new version during a write propagation:
If it's not the tail node, it marks the version as dirty and forwards the write.
If it is the tail node, it marks the version as clean (committed) and sends an acknowledgement backward through the chain.
Upon receiving an acknowledgement for a version, a node marks that version as clean and can discard older versions of the object.

When a node receives a read request:
If the latest version is clean, it returns that version.
If the latest version is dirty, it queries the tail for the latest committed version number and returns that specific version, 
which it is guaranteed to possess. This ensures strong consistency by serializing reads with the tail.

In CRAQ, each object has a two-part identifier consisting of:
Chain identifier (chain_id)
Key identifier (key_id)
The complete object identifier looks like: chain_id:key_id

Configuration Strategy 1: Implicit Datacenters & Global Chain Size
num_datacenters chain_size
In this approach:
You specify how many datacenters should store the chain, but not specifically which ones
The system uses consistent hashing with datacenter identifiers to determine which datacenters will store the chain
All selected datacenters use the same chain size (number of nodes per datacenter)

For example, if you specify 3 5, the chain will be stored across 3 datacenters (selected automatically via consistent hashing),
with each datacenter using 5 nodes in its portion of the chain=> Total length of chain will be 15.


Does that mean CRAQ is strictly more powerful than Raft &c?
No.
All CRAQ replicas have to participate for any write to commit.
If a node isn't reachable, CRAQ must wait.
So not immediately fault-tolerant in the way that ZK and Raft are.
CR has the same limitation.

How can we safely make use of a replication system that can't handle partition?
A single "configuration manager" must choose head, chain, tail.
Everyone (servers, clients) must obey or stop.
Regardless of who they locally think is alive/dead.
A configuration manager is a common and useful pattern.
It's the essence of how GFS (master) and VMware-FT (test-and-set server) work.
Usually Paxos/Raft/ZK for config service, data sharded over many replica groups, CR or something else fast for each replica group.

Q: How does CRAQ cope with network partition and prevent split brain?

A: CRAQ (and Chain Replication) do not themselves have a defense against partitions and split brain. In the short term, if a chain node
doesn't respond, the other chain members have to wait. CRAQ and CR depend on a separate configuration service, which decides which
servers make up each chain. The configuration service monitors the servers to form an opinion of who is alive, and every time a server
seems to be unreachable, the configuration service decides which servers in the chain are still alive and tells those servers (and the
clients) the new chain setup. Configuration services are typically built with Paxos or Raft or (in CRAQ's case) ZooKeeper so that they
are fault tolerant and so that they themselves avoid split brain despite partition. At a high level, both GFS and VMware-FT follow this
pattern (GFS's master monitors server liveness and picks primaries; VMware-FT's test-and-set service picks the sole server if one dies).

Q: What are the tradeoffs of Chain Replication vs Raft or Paxos?

A: CR and CRAQ are likely to be faster than Raft because the CR head does less work than the Raft leader: the CR head sends writes to just one
replica, while the Raft leader must send all operations to all followers. CR has a performance advantage for reads as well, since it
serves them from the tail (not the head), while the Raft leader must serve all client requests.

However, Raft/Paxos and CR/CRAQ differ significantly in their failure properties. Raft (and Paxos and ZooKeeper) can continue operating
(with no pauses at all) even if a minority of nodes are crashed, slow, unreliable, or partitioned. A CRAQ or CR chain must stop if something
like that goes wrong, and wait for a configuration manager to decide how to proceed. On the other hand the post-failure situation is
significantly simpler in CR/CRAQ.

Q: Would Chain Replication be significantly faster or slower than the kind of primary/backup used in GFS?

A: If there are just two replicas, there's probably not much difference. Though maybe CR would be faster for writes since the tail
can send responses directly to the client; in a classic primary/backup scheme, the primary has to wait for the backup to acknowledge a write
before the primary responds to the client.

If there are three or more replicas, the primary in a classic primary/backup system has to send each write to each of the replicas. If
the write data is big, these network sends could put a significant load on the primary. Chain Replication spreads this networking load over all
the replicas, so CR's head node might be less of a performance bottleneck than a classic primary. On the other hand maybe the client-observed 
latency for writes would be higher in CR.

Q: The paper's Introduction mentions that one could use multiple chains to solve the problem of intermediate chain nodes not serving
reads. What does this mean?

A: In Chain Replication, only the head and tail directly serve client requests; the other replicas help fault tolerance but not performance.
Since the load on the head and tail is thus likely to be higher than the load on intermediate nodes, you could get into a situation where
performance is bottlenecked by head/tail, yet there is plenty of idle CPU available in the intermediate nodes. CRAQ exploits that idle CPU
by moving the read work to them.

The Introduction is referring to this alternate approach. A data center will probably have lots of distinct CR chains, each serving a
fraction (shard) of the objects. Suppose you have three servers (S1, S2, and S3) and three chains (C1, C2, C3). Then you can have the three
chains be:

  C1: S1 S2 S3
  C2: S2 S3 S1
  C3: S3 S1 S2

Now, assuming activity on the three chains is roughly equal, the load on the three servers will also be roughly equal. In particular the load of
serving client requests (head and tail) will be roughly equally divided among the three servers.

This is a pretty reasonable arrangement; CRAQ is only better if it
turns out that some chains see more load than others.

Q: Is it enough to make CR strongly consistent if we restrict one client
to only read from the same node throughout a session?

A: This is not OK in CR, for at least two reasons. First, reading from a CR node other than the tail may return a write that has not
committed; if some nodes then failed, that un-committed write might be lost. It's not legal for a read to see a write that essentially never
occured. The second problem is that, if clients compare notes (which they are allowed to do under linearizability), they may see that one
client sees a write, but another client that reads later in real time does not see the write. That's a violation of linearizability as well.

Q: Is the failure model for CRAQ non-Byzantine?

A: CRAQ cannot handle Byzantine failures. Just fail-stop failures.

Few systems have a good story for Byzantine failures, and typically have to make sacrifices in performance or flexibility when they do.
There are two main approaches I'm aware of. First, systems derived
from a paper titled Practical Byzantine Fault Tolerance (PBFT) by Castro and Liskov; PBFT is like Raft but has more rounds of
communication and uses cryptography. Second, systems in which clients can directly check the correctness of results that servers return,
typically by use of cryptographic hashes or signatures. This can be tricky because clients need to defend against a server that returns
data whose signature or hash is correct, but is not the latest value. Systems like this include SUNDR and Bitcoin.

Q: Is Chain Replication used by other systems?

A: Some examples: Amazon's EBS, Ceph's Rados, Google's Parameter Server, COPS, and FAWN.

Q: What alternatives exist to the CRAQ model?

A: People use Chain Replication (though not CRAQ) fairly frequently.

People use quorum systems such as Paxos and Raft very frequently (e.g.ZooKeeper, Google Chubby and Spanner and Megastore).

There are lots of primary/backup replication schemes that you can view as similar to a Chain Replication chain with just two nodes, or with the
primary sending to all replicas directly (no chain). GFS is like this.

The main technique that people use to keep strong consistency but allow replicas to serve reads is leases.

Q: Why does CRAQ keep the old clean object when it sees a write
and creates a new dirty object?

A: Suppose a node has clean version 1, and dirty version 2. If the node receives a read from a client, it sends a "version query" to the
tail. If the tail hasn't seen version 2, the tail will reply with version number 1. The node should then reply with the data for version
1. So it has to hold on to a copy of version 1's data until version 2 commits.

CRAQ ACROSS MULTIPLE DATA CENTERS

CRAQ’s ability to read from any node improves its latency when chains stretch across the wide-area.
Clients can choose to read from nodes closer to them, whereas in traditional CR clients could read from the tail nodes
which could potentially be distant.
We can carefully select the order of datacenters that comprises a chain and make sure we are not crossing the boundaries of 
datacenters multiple times within the same chain.
The latency of write operations over wide-area links will increase as more datacenters are added 
to the chain however this allows writes to be pipelined down the chain which vastly improves write throughput.

How Zookeeper Helps in Coordination?

CRAQ nodes use ZooKeeper to manage cluster membership and metadata, receiving notifications when nodes are added/removed or when relevant metadata changes.
However, managing membership and metadata across multiple datacenters using standard ZooKeeper is challenging. Vanilla ZooKeeper is not 
optimized for wide-area networks; while it scales local reads within a datacenter, its lack of topology awareness causes redundant coordination messages to be
sent multiple times across the WAN, negatively impacting performance. The current CRAQ implementation mitigates this by ensuring nodes interact with local
ZooKeeper replicas and receive only relevant notifications.

To address the issue of redundant cross-datacenter ZooKeeper traffic more fundamentally, a hierarchical ZooKeeper structure is suggested. 
This involves having local ZooKeeper instances within each datacenter (potentially with leader election) and a representative from each 
local instance participating in a global ZooKeeper instance, with separate mechanisms coordinating data sharing between the local and global layers.
